# 12 - Observability

*Version: 2.0.0*
*Author: Architecture Team*
*Created: 2025-01-27*

## Changelog

- 2.0.0 (2026-02-11): Added frontend tracking (X-Frontend-ID), improved health checks, clarified production deployment
- 1.0.0 (2025-01-27): Initial generic observability standard

---

## Context

When something breaks in production, the first question is always "what happened?" If the answer requires SSHing into a server and grepping unstructured log files, debugging takes hours instead of minutes. This document ensures that every application produces structured, queryable, correlated logs from day one — so that tracing a request from client to database to response is a single query, not an investigation.

The core design is built around the `X-Request-ID` header: every request gets a unique identifier that propagates through all log entries, downstream calls, and the response. Combined with `X-Frontend-ID` (which identifies whether the request came from web, CLI, mobile, or Telegram), any production issue can be traced to its source and followed through the entire system. This is implemented in the skeleton code itself, not deferred to production infrastructure.

The document separates skeleton features (structured logging with structlog, request context middleware, health check endpoints) from production deployment (Prometheus, Loki, Grafana). The skeleton ships with everything needed to produce good logs and health signals; the production stack is infrastructure that consumes them. Health endpoints (`/health`, `/health/ready`, `/health/detailed`) follow a standard pattern that integrates with both bare-metal (21) and Azure (22) deployment health probes, and feed into the security logging requirements defined in authentication (09) and security standards (17).

---

## Overview

This document defines observability standards for the skeleton and applications built from it.

**Core Skeleton Features** (implemented in code):
- Structured logging with structlog
- Request context middleware (X-Request-ID, X-Frontend-ID)
- Health check endpoints with dependency checks
- Response timing headers

**Production Deployment** (infrastructure, not in skeleton code):
- Prometheus for metrics collection
- Loki for log aggregation
- Grafana for dashboards and alerting

---

## Request Context

### Standard Headers

| Header | Direction | Purpose |
|--------|-----------|---------|
| `X-Request-ID` | In/Out | Unique request identifier for tracing |
| `X-Frontend-ID` | In | Source frontend identifier |
| `X-Response-Time` | Out | Response duration in milliseconds |

### X-Request-ID

Every request receives a unique identifier:
- Generated by middleware if not provided
- Propagated to all downstream calls
- Included in all log entries
- Returned in response headers

```python
# Accessing in endpoint
request_id = request.state.request_id
```

### X-Frontend-ID

Identifies the source frontend for debugging and analytics:

| Value | Description |
|-------|-------------|
| `web` | Web browser frontend |
| `cli` | Command-line interface |
| `mobile` | Mobile application |
| `telegram` | Telegram bot |
| `api` | Direct API integration |
| `internal` | Internal service calls |
| `unknown` | Not provided or unrecognized |

```python
# Accessing in endpoint
frontend = request.state.frontend
```

**Implementation**: Frontends should send this header with every request. The middleware validates against known values and defaults to "unknown" if not recognized.

### Response Timing

The `X-Response-Time` header returns the server-side processing time in milliseconds. This helps identify slow requests without requiring server log access.

---

## Logging

### Standard: structlog

All Python applications use structlog for structured logging.

Rationale:
- Structured JSON output for parsing
- Context binding across request
- Compatible with standard logging
- Easy to query and analyze

### Log Format

Production logs output as JSON with these fields:
- `timestamp`: ISO8601 UTC
- `level`: DEBUG, INFO, WARNING, ERROR, CRITICAL
- `logger`: Module/component name
- `message`: Human-readable message
- `request_id`: Correlation ID for request tracing
- `frontend`: Source frontend identifier
- Additional context fields as needed

Development logs may use human-readable format for convenience.

### Log Levels

| Level | Usage |
|-------|-------|
| DEBUG | Detailed diagnostic information for troubleshooting |
| INFO | Normal operation events worth recording |
| WARNING | Unexpected conditions that don't prevent operation |
| ERROR | Failures that affect single operation |
| CRITICAL | Failures that affect system availability |

### What to Log

**Always log:**
- Application startup and shutdown
- Configuration loaded (without secrets)
- External service calls (endpoint, duration, status)
- Database query performance (slow queries)
- Authentication events
- Error conditions with context

**Never log:**
- Passwords or tokens
- Full credit card numbers
- Personal data beyond identifiers
- Request/response bodies with sensitive data

### Log Storage

All logs are written to a single JSONL file: `logs/system.jsonl`. Filter by the `source`
field to isolate logs from a specific origin.

#### Structured Fields

Every JSON log record contains:

| Field | Description | Presence |
|-------|-------------|----------|
| `timestamp` | ISO 8601 UTC timestamp | Always |
| `level` | Log level (debug, info, warning, error, critical) | Always |
| `logger` | Module path (e.g., `modules.backend.api.health`) | Always |
| `event` | Log message | Always |
| `func_name` | Function that emitted the log | Always |
| `lineno` | Line number in source file | Always |
| `source` | Origin context (web, cli, tui, telegram, api, tasks, internal) | When set explicitly |
| `request_id` | Request correlation ID | In HTTP request context |

Additional fields are added by callers via extra kwargs or structlog context binding.

#### Source Values

Source is always set explicitly — never guessed from logger names.

| Source | Set by |
|--------|--------|
| `web` | Middleware, from `X-Frontend-ID: web` header |
| `cli` | Entry point binding in `cli.py`, `chat.py` |
| `tui` | Entry point binding in `tui.py` |
| `mobile` | Middleware, from `X-Frontend-ID: mobile` header |
| `telegram` | `log_with_source()` in telegram handlers |
| `api` | Middleware, from `X-Frontend-ID: api` header |
| `tasks` | `log_with_source()` in background tasks |
| `internal` | `log_with_source()` in internal services |

#### File Rotation

- **Max size**: 10MB per file (configurable in `logging.yaml`)
- **Backups**: 5 rotated files kept (`system.jsonl.1`, `system.jsonl.2`, etc.)
- **Encoding**: UTF-8

#### Troubleshooting with Log Files

```bash
# View recent errors
jq 'select(.level == "error")' logs/system.jsonl | tail -20

# Find all logs for a specific request
jq 'select(.request_id == "abc-123")' logs/system.jsonl

# Filter by source
jq 'select(.source == "telegram")' logs/system.jsonl

# Watch logs in real-time
tail -f logs/system.jsonl | jq .

# Count errors
jq 'select(.level == "error")' logs/system.jsonl | wc -l
```

#### Explicit Source Logging

For non-HTTP contexts (no middleware), set source explicitly:

```python
from modules.backend.core.logging import get_logger, log_with_source

logger = get_logger(__name__)

# In HTTP context: source is set automatically by middleware from X-Frontend-ID header
logger.info("User logged in", extra={"user_id": 123})

# Outside HTTP context: set source explicitly
log_with_source(logger, "tasks", "warning", "Slow query", query_ms=150)
```

### Centralized Logging (Production)

For production deployments, aggregate logs using Loki:

```yaml
# Example Loki configuration
# loki:
#   url: http://loki:3100/loki/api/v1/push
#   labels:
#     app: ${APP_NAME}
#     env: ${APP_ENV}
```

Use Promtail or similar agent to ship logs from `logs/` to Loki.

---

## Health Checks

### Endpoint Structure

Three health endpoints implemented in `modules/backend/api/health.py`:

**`GET /health`** - Liveness
- Returns 200 if process is running
- No dependency checks
- Used by process monitors (Kubernetes liveness probe)
- Response: `{"status": "healthy"}`

**`GET /health/ready`** - Readiness
- Returns 200 if ready to serve traffic
- Checks critical dependencies (database, Redis) in parallel
- Used by load balancers (Kubernetes readiness probe)
- Returns 503 if any configured dependency is unhealthy

**`GET /health/detailed`** - Component Status
- Returns comprehensive status of each component
- Includes latency measurements
- Should be protected by authentication in production
- Used for debugging and monitoring dashboards

### Response Format

```json
{
  "status": "healthy",
  "application": {
    "name": "my-app",
    "env": "production",
    "version": "1.0.0"
  },
  "checks": {
    "database": {
      "status": "healthy",
      "latency_ms": 5
    },
    "redis": {
      "status": "healthy",
      "latency_ms": 1
    }
  },
  "timestamp": "2026-02-11T12:00:00"
}
```

### Status Values

| Status | Meaning |
|--------|---------|
| `healthy` | Component is working correctly |
| `unhealthy` | Component is failing |
| `not_configured` | Component not enabled (OK for optional dependencies) |

### Health Check Implementation

- Checks run in parallel for fast response
- Use simple queries (SELECT 1, PING)
- Total response time should be < 1 second
- Fail open for non-critical dependencies

---

## Metrics (Production)

### Application Metrics

Track these metrics for all services:

**Request metrics:**
- Request count by endpoint, method, status, and frontend
- Request duration (p50, p95, p99)
- Request size
- Error rate by type

**System metrics:**
- CPU usage
- Memory usage
- Disk usage
- Open file descriptors

**Business metrics:**
- Active users
- Operations per period
- Queue depths
- Cache hit rates

### Metric Format

Use Prometheus format for metrics:
- Counter for cumulative values
- Gauge for current values
- Histogram for distributions
- Labels for dimensions (including `frontend` from X-Frontend-ID)

### Prometheus Integration (Production)

For production deployments, expose a `/metrics` endpoint:

```python
# Example: Add prometheus-fastapi-instrumentator
# from prometheus_fastapi_instrumentator import Instrumentator
# Instrumentator().instrument(app).expose(app)
```

This is not included in the skeleton by default. Add when deploying to production with Prometheus.

---

## Error Tracking

### Error Capture

All unhandled exceptions captured with:
- Full stack trace
- Request context (URL, method, user, request_id, frontend)
- Environment information
- Application version

### Error Grouping

Group related errors:
- By exception type and location
- By error message pattern
- Track occurrence count and timeline

### Error Alerting

Alert on:
- New error types (not seen before)
- Error rate exceeds threshold
- Critical errors (always)

---

## Alerting (Production)

### Alert Categories

| Category | Response Time | Examples |
|----------|---------------|----------|
| Critical | Immediate | Service down, data loss risk |
| Warning | Hours | High error rate, resource pressure |
| Info | Next business day | Unusual patterns, approaching limits |

### Alert Channels

- Critical: SMS/phone + email
- Warning: Email + chat (Slack/Discord)
- Info: Email or dashboard

### Grafana Alerting

For production deployments, configure alerts in Grafana:

```yaml
# Example alert rule
# - alert: HighErrorRate
#   expr: rate(http_requests_total{status=~"5.."}[5m]) > 0.1
#   for: 5m
#   labels:
#     severity: warning
#   annotations:
#     summary: High error rate detected
```

---

## Performance Monitoring

### Slow Query Detection

Log queries exceeding threshold:
- Default threshold: 100ms
- Include query (without parameters), duration, caller
- Review periodically, add indexes or optimize

### Slow Request Detection

Log requests exceeding threshold:
- Default threshold: 1 second
- Include endpoint, method, duration, user, frontend
- Breakdown by component (database, external calls)

### Resource Monitoring

Monitor and alert on:
- Database connection pool exhaustion
- Redis memory usage
- Disk space
- Process memory growth

---

## Debugging

### Debug Mode

Applications support debug mode:
- Enabled via `APP_DEBUG=true` environment variable
- More verbose logging (DEBUG level)
- Detailed error responses (development only)
- Performance profiling available

### Log Level Override

Runtime log level changes:
- Via `LOG_LEVEL` environment variable (requires restart)
- Via admin API (no restart, temporary) - implement as needed

### Request Debugging

For specific request troubleshooting:
- Use X-Request-ID to trace through logs
- Filter logs by X-Frontend-ID to isolate frontend issues
- Check X-Response-Time header for performance issues

---

## Dashboards (Production)

### Essential Dashboards

**Operations Dashboard:**
- Service health status (from /health endpoints)
- Request rate and latency by frontend
- Error rate by type and frontend
- Resource utilization

**Business Dashboard:**
- Active users by frontend
- Key business metrics
- Trend comparisons

### Grafana Setup

For production deployments, use Grafana with:
- Prometheus as metrics data source
- Loki as logs data source
- Pre-built dashboards for FastAPI applications

---

## Production Deployment Stack

### Recommended Stack

| Component | Tool | Purpose |
|-----------|------|---------|
| Metrics | Prometheus | Time-series metrics collection |
| Logs | Loki | Log aggregation and querying |
| Dashboards | Grafana | Visualization and alerting |
| Log Shipping | Promtail | Ship logs to Loki |

### Deployment Notes

This stack is **not included in the skeleton code**. It's infrastructure that should be deployed separately:

1. **Development**: Use local logs in `logs/`, no metrics infrastructure needed
2. **Staging/Production**: Deploy Prometheus + Loki + Grafana stack, configure log shipping

The skeleton provides the hooks (structured logs, health endpoints, request context) that integrate with this stack.

---

## Log Retention

### Retention Policy

| Log Type | Retention |
|----------|-----------|
| Application logs | 30 days |
| Access logs | 90 days |
| Audit logs | 1 year minimum |
| Debug logs | 7 days |

Adjust based on compliance requirements.

### Log Archival

After retention period:
- Compress and archive to cold storage
- Or delete if not required

Implement automated cleanup to prevent disk exhaustion.

---

## Runbooks

### Runbook Content

For each alert type, document:
- What the alert means
- Potential causes
- Investigation steps
- Resolution procedures
- Escalation path

### Runbook Location

Store runbooks with documentation:
- Version controlled
- Linked from alerts
- Reviewed and updated regularly

---

## Future Enhancements

### Telemetry Debug API (Not Implemented)

A debugging endpoint that enables verbose logging for specific requests without affecting global log levels.

**Endpoint**: `POST /api/telemetry/debug`

**Purpose**: When debugging production issues, enable detailed logging for specific sessions without:
- Enabling debug logging globally (too noisy)
- Affecting other users' performance
- Restarting the application

**Proposed Request**:

```json
{
  "enable": true,
  "duration_minutes": 15,
  "frontend_id": "web"
}
```

**Proposed Response**:

```json
{
  "debug_token": "abc123...",
  "expires_at": "2026-02-11T12:15:00Z",
  "instructions": "Add X-Debug-Token header to requests"
}
```

**How It Would Work**:

1. Client requests debug token with duration and optional frontend filter
2. Server generates token, stores in Redis with TTL
3. Client includes `X-Debug-Token: abc123` header in subsequent requests
4. Middleware detects token, enables verbose logging for that request:
   - DEBUG level logging instead of INFO
   - Detailed timing breakdown per middleware/handler
   - Database query details (without sensitive parameters)
   - Request/response bodies (sanitized)
5. Token expires automatically after duration

**Implementation Requirements**:

- Token generation and storage (Redis recommended)
- Middleware enhancement to check for debug token
- Output sanitization (strip passwords, tokens, PII)
- Rate limiting to prevent abuse
- Audit logging (who enabled debug mode, when)
- Authentication required (admin/developer role)

**Security Considerations**:

- Endpoint must require authentication
- Debug output must be sanitized to prevent data leakage
- Tokens should have short TTLs (max 1 hour recommended)
- Audit trail for compliance

**Why Deferred**:

Current debugging capabilities are sufficient for most use cases:
- `APP_DEBUG=true` for development/staging environments
- `X-Request-ID` for tracing specific requests through logs
- `X-Frontend-ID` for filtering logs by source
- `/health/detailed` for dependency status

Implement this enhancement when:
- Production debugging becomes frequent
- Multiple developers need isolated debug sessions
- Compliance requires audit trails for debug access
